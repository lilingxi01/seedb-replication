{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdef1e3-c6b3-496a-a77b-6e0eaa311986",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ddff71-e30d-4f4e-a927-ca8a6f6c385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from `.env` file for database URL. Check README for the setup guide.\n",
    "load_dotenv()\n",
    "\n",
    "# Ignore SQLAlchemy version warning for this project.\n",
    "os.environ['SQLALCHEMY_SILENCE_UBER_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5523b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7869a9-8807-4fc4-af07-9f90a64ea7f4",
   "metadata": {},
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535ca410",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL = os.environ['DATABASE_URL']\n",
    "if not DATABASE_URL:\n",
    "    print('Check README file for environment setup')\n",
    "    sys.exit(1)\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68dff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connection successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = engine.execute(\"SELECT 1\")\n",
    "    print(\"âœ… Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"ðŸ”´ Connection failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7caa4b-34b9-4acd-a4d4-e41d3e6a618e",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ac79c6-813d-439f-a4d8-04bd9064db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "delta = 0.0001\n",
    "k = 5\n",
    "num_phases = 5  # This value should align with the data. It should not be changed unless you build your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b124be-f3a9-4d19-87d5-950840e6ab28",
   "metadata": {},
   "source": [
    "# Math Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0aa2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    epsilon = 1e-10  # A small-enough constant for avoiding computational error.\n",
    "    p = np.where(p == 0, epsilon, p)\n",
    "    q = np.where(q == 0, epsilon, q)\n",
    "    return np.sum(p * np.log(p / q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732f7555-0f7e-4b8c-8002-04a07eff90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_m(m: int, N: int):\n",
    "    \"\"\"\n",
    "    Calculate the epsilon value for the confidence interval pruning method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        The number of items in the sample.\n",
    "    N : int\n",
    "        The number of items in the population.\n",
    "    \"\"\"\n",
    "    upper_left = 1 - (m - 1) / N\n",
    "    # Using log log m is problematic when m=1. So we change it to log m for now.\n",
    "    upper_right = 2 * np.log(m) + np.log((np.pi ** 2) / (3 * delta))\n",
    "    bottom = 2 * m\n",
    "    return np.sqrt(upper_left * upper_right / bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56dda3-a9ac-4706-8c4a-2bfa25c1252c",
   "metadata": {},
   "source": [
    "# Query Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5f9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# These are all available candidates for (attribute, measurement, function) combinations.\n",
    "dimensions = ['workclass', 'education', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
    "measurements = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "aggregate_functions = ['count', 'max', 'min', 'sum', 'avg']\n",
    "\n",
    "# Iterate through the cross product of three lists to build all views.\n",
    "# We use list instead of dict or set here for the ordering consistency. The hashing might cause ordering issues.\n",
    "views = list()\n",
    "for dimension, measurement, function in itertools.product(dimensions, measurements, aggregate_functions):\n",
    "    views.append((dimension, measurement, function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fbbbc80-59dd-4cf4-a172-7c269e8fdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(candidate_views: list):\n",
    "    \"\"\"\n",
    "    This function is used to generate queries based off the current candidate views.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_views\n",
    "        The modified dictionary of views after pruning views in the end of every phase.\n",
    "    \"\"\"\n",
    "    married_query_list = []\n",
    "    unmarried_query_list = []\n",
    "    for (attribute, measurement, function) in candidate_views:\n",
    "        married_query_list.append(f\"SELECT {attribute}, {function}({measurement}) AS {function}_{measurement} FROM married_data WHERE {attribute} IS NOT NULL GROUP BY {attribute};\")\n",
    "        unmarried_query_list.append(f\"SELECT {attribute}, {function}({measurement}) AS {function}_{measurement} FROM unmarried_data WHERE {attribute} IS NOT NULL GROUP BY {attribute};\")\n",
    "    return married_query_list, unmarried_query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45970da4-bc1e-43d1-acfa-36fdb5a40d35",
   "metadata": {},
   "source": [
    "# Baseline (Non-optimizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032b0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline():\n",
    "    # Generate queries based on all views (no partitioning).\n",
    "    raw_queries_married, raw_queries_unmarried = generate_queries(views)\n",
    "\n",
    "    # Query data from database and store them into `results` variable.\n",
    "    results = []\n",
    "    for raw_query_married, raw_query_unmarried in zip(raw_queries_married, raw_queries_unmarried):\n",
    "        # Run raw query on both married and unmarried data.\n",
    "        df_married = pd.read_sql_query(raw_query_married, engine)\n",
    "        df_unmarried = pd.read_sql_query(raw_query_unmarried, engine)\n",
    "        # Set them as a tuple for later utility computation.\n",
    "        results.append((df_married, df_unmarried))\n",
    "\n",
    "    # Storing scores across phases, where key = (a,m,f) and value = KL.\n",
    "    view_scores = dict()\n",
    "\n",
    "    # Align views with results, and compute utility scores (KL).\n",
    "    for (attribute, measurement, function), (df_married, df_unmarried) in zip(views, results):\n",
    "        # Collects unique values of this attribute from both married and unmarried dataframes.\n",
    "        all_attribute_values = set(df_married[attribute].unique()).union(set(df_unmarried[attribute].unique()))\n",
    "        \n",
    "        # Organize data and ensure all possible attribute values are included using reindex function.\n",
    "        grouped_married = df_married.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "        grouped_unmarried = df_unmarried.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "        \n",
    "        # Compute sum for normalization.\n",
    "        total_married = grouped_married[f'{function}_{measurement}'].sum()\n",
    "        total_unmarried = grouped_unmarried[f'{function}_{measurement}'].sum()\n",
    "\n",
    "        # Get KL score after normalizing data and store it into `view_scores` dict.\n",
    "        p = (grouped_married / total_married).fillna(0).values.flatten()\n",
    "        q = (grouped_unmarried / total_unmarried).fillna(0).values.flatten()\n",
    "        score = kl_divergence(p, q)\n",
    "        view_scores[(attribute, measurement, function)] = score\n",
    "\n",
    "    # Print top-k results.\n",
    "    top_scores = sorted(view_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    for key, score in top_scores:\n",
    "        print(f\"View: {key}, Score: {score}\")\n",
    "\n",
    "    return top_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccd765-3671-45bd-9d8c-cccbe6ee7601",
   "metadata": {},
   "source": [
    "# Section 4.1: Sharing-based Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a3916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can generated the combined queries.\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "function_lst = ['SUM', 'COUNT', 'AVG', 'MAX', 'MIN']\n",
    "\n",
    "select_pattern = r\"SELECT\\s+(.*?)\\s+FROM\"\n",
    "from_pattern = r\"FROM\\s+(.*?)(?:\\s+WHERE|\\s+GROUP\\s+BY|$)\"\n",
    "where_pattern = r\"WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|$)\"\n",
    "group_by_pattern = r\"GROUP\\s+BY\\s+(.*)\"\n",
    "\n",
    "\n",
    "def parseQuery(query):\n",
    "    select_match = re.search(select_pattern, query, re.IGNORECASE)\n",
    "    from_match = re.search(from_pattern, query, re.IGNORECASE)\n",
    "    where_match = re.search(where_pattern, query, re.IGNORECASE)\n",
    "    group_by_match = re.search(group_by_pattern, query, re.IGNORECASE)\n",
    "\n",
    "    select_items = select_match.group(1) if select_match else \"\"\n",
    "    from_items = from_match.group(1) if from_match else \"\"\n",
    "    where_items = where_match.group(1) if where_match else \"\"\n",
    "    group_by_items = group_by_match.group(1) if group_by_match else \"\"\n",
    "\n",
    "    return select_items, from_items, where_items, group_by_items\n",
    "\n",
    "\n",
    "def parseSelectItems(select_items):\n",
    "    components = select_items.split(',')\n",
    "    attribute = None\n",
    "    measures, functions = [], []\n",
    "\n",
    "    for component in components:\n",
    "        component = component.strip()\n",
    "        component_upper = component.upper()\n",
    "        for func in function_lst:\n",
    "            if func.upper() in component_upper:\n",
    "                measure_match = re.search(rf\"{func}\\((.*?)\\)\", component, re.IGNORECASE)\n",
    "                if measure_match:\n",
    "                    measure = measure_match.group(1)\n",
    "                    measures.append(measure)\n",
    "                    functions.append(func)\n",
    "                break\n",
    "        else:\n",
    "            attribute = component\n",
    "\n",
    "    return attribute, measures, functions\n",
    "\n",
    "def combineAggregates(queries, partition=None):\n",
    "    # Group queries by FROM and WHERE clauses for potential combination\n",
    "    query_groups = defaultdict(list)\n",
    "    for query in queries:\n",
    "        select_items, from_clause, where_clause, group_by_items = parseQuery(query.rstrip(';'))\n",
    "        attribute, measures, functions = parseSelectItems(select_items)\n",
    "        key = (from_clause, where_clause)\n",
    "        query_groups[key].append((group_by_items, measures, functions, attribute))\n",
    "\n",
    "    combined_queries = []\n",
    "    for (from_clause, where_clause), group in query_groups.items():\n",
    "        combined_by_group = defaultdict(list)\n",
    "        for group_by_items, measures, functions, attribute in group:\n",
    "            combined_by_group[group_by_items].append((measures, functions, attribute))\n",
    "\n",
    "        for group_by_items, details in combined_by_group.items():\n",
    "            select_parts = []\n",
    "            for measures, functions, attribute in details:\n",
    "                for measure, function in zip(measures, functions):\n",
    "                    select_parts.append(f\"{function}({measure}) AS {function}_{measure}\")\n",
    "            select_clause = ', '.join(select_parts)\n",
    "            if partition is not None:\n",
    "                partition_clause = f\"partition_id = {partition}\"\n",
    "                where_clause = f\"{where_clause} AND {partition_clause}\" if where_clause else partition_clause\n",
    "            where_clause = f\"WHERE {where_clause}\" if where_clause else \"\"\n",
    "            group_by_clause = f\"GROUP BY {group_by_items}\" if group_by_items else \"\"\n",
    "            query = f\"SELECT {group_by_items}, {select_clause} FROM {from_clause} {where_clause} {group_by_clause};\"\n",
    "            combined_queries.append(query)\n",
    "\n",
    "    return combined_queries\n",
    "\n",
    "\n",
    "def decomposeAggTable(dimensions_name, combined_dataframe, individual_list):\n",
    "    measurements = set(col.split('_')[1] for col in combined_dataframe.columns if '_' in col)\n",
    "\n",
    "    # Create a dictionary to store each separate DataFrame\n",
    "    separated_tables = {}\n",
    "\n",
    "    for column in combined_dataframe.columns:\n",
    "        if column != dimensions_name:\n",
    "            # Properly split the column name\n",
    "            parts = column.split('_')\n",
    "            func = parts[0]  # The function is always the first part\n",
    "            measure = '_'.join(parts[1:])  # The rest is the measurement name\n",
    "\n",
    "            table_name = f\"{func}_{measure}\"  # Create a unique table name\n",
    "\n",
    "            # Create a new DataFrame for this specific measurement and function\n",
    "            separated_tables[table_name] = combined_dataframe[[dimensions_name, column]].copy()\n",
    "            separated_tables[table_name].rename(columns={column: table_name}, inplace=True)\n",
    "\n",
    "    # Printing or exporting the separated tables\n",
    "    for table_name, df in separated_tables.items():\n",
    "        individual_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4e3cd4-17d5-4c8c-95ad-2fcd701ad664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharing_based_optimizations():\n",
    "    # Get raw queries (separate statements).\n",
    "    raw_queries_married, raw_queries_unmarried = generate_queries(views)\n",
    "\n",
    "    # Combine statements together (Section 4.1).\n",
    "    combined_queries_married = combineAggregates(raw_queries_married)\n",
    "    combined_queries_unmarried = combineAggregates(raw_queries_unmarried)\n",
    "\n",
    "    # Run combined queries for both married and unmarried data.\n",
    "    agg_opt_res = []\n",
    "    for i in range(len(combined_queries_married)):\n",
    "        df_married_agg_opt = pd.read_sql_query(combined_queries_married[i], engine)\n",
    "        df_unmarried_agg_opt = pd.read_sql_query(combined_queries_unmarried[i], engine)\n",
    "        agg_opt_res.append((df_married_agg_opt, df_unmarried_agg_opt))\n",
    "\n",
    "    # Decompose combined queries back into original views.\n",
    "    de_married_list = []\n",
    "    de_unmarried_list = []\n",
    "    for i in range(len(agg_opt_res)):\n",
    "        dimensions_name = agg_opt_res[i][0].columns[0]\n",
    "        married_combined_df = agg_opt_res[i][0]\n",
    "        unmarried_combined_df = agg_opt_res[i][1]\n",
    "        decomposeAggTable(dimensions_name, married_combined_df, de_married_list)\n",
    "        decomposeAggTable(dimensions_name, unmarried_combined_df, de_unmarried_list)\n",
    "\n",
    "    # Storing scores across phases, where key = (a,m,f) and value = KL.\n",
    "    view_scores = dict()\n",
    "\n",
    "    # Align views with results, and compute utility scores (KL).\n",
    "    for (attribute, measurement, function), df_married, df_unmarried in zip(views, de_married_list, de_unmarried_list):            \n",
    "        # Collects unique values of this attribute from both married and unmarried dataframes.\n",
    "        all_attribute_values = set(df_married[attribute].unique()).union(set(df_unmarried[attribute].unique()))\n",
    "        \n",
    "        # Organize data and ensure all possible attribute values are included using reindex function.\n",
    "        grouped_married = df_married.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "        grouped_unmarried = df_unmarried.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "        \n",
    "        # Compute sum for normalization.\n",
    "        total_married = grouped_married[f'{function}_{measurement}'].sum()\n",
    "        total_unmarried = grouped_unmarried[f'{function}_{measurement}'].sum()\n",
    "\n",
    "        # Get KL score after normalizing data and store it into `view_scores` dict.\n",
    "        p = (grouped_married / total_married).fillna(0).values.flatten()\n",
    "        q = (grouped_unmarried / total_unmarried).fillna(0).values.flatten()\n",
    "        score = kl_divergence(p, q)\n",
    "        view_scores[(attribute, measurement, function)] = score\n",
    "\n",
    "    # Print top-k results.\n",
    "    top_scores = sorted(view_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    for key, score in top_scores:\n",
    "        print(f\"View: {key}, Score: {score}\")\n",
    "\n",
    "    return top_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d8b2b",
   "metadata": {},
   "source": [
    "# Section 4.2: Pruning-based Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a98fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_based_optimizations():\n",
    "    # Make a copy of the views dictionary so that it won't change the original data.\n",
    "    candidate_views = copy.deepcopy(views)\n",
    "\n",
    "    # Storing scores across phases, where key = (a,m,f) and value = list[KL].\n",
    "    view_scores = defaultdict(list)\n",
    "    \n",
    "    for phase in range(num_phases):\n",
    "        # Get raw queries (separate statements) from candidate views.\n",
    "        raw_queries_married, raw_queries_unmarried = generate_queries(candidate_views)\n",
    "\n",
    "        # Combine statements together (Section 4.1) with data partitioning.\n",
    "        combined_queries_married = combineAggregates(raw_queries_married, partition=phase)\n",
    "        combined_queries_unmarried = combineAggregates(raw_queries_unmarried, partition=phase)\n",
    "\n",
    "        # Run combined queries for both married and unmarried data.\n",
    "        agg_opt_res = []\n",
    "        for i in range(len(combined_queries_married)):\n",
    "            df_married_agg_opt = pd.read_sql_query(combined_queries_married[i], engine)\n",
    "            df_unmarried_agg_opt = pd.read_sql_query(combined_queries_unmarried[i], engine)\n",
    "            agg_opt_res.append((df_married_agg_opt, df_unmarried_agg_opt))\n",
    "\n",
    "        # Decompose combined queries back into original views.\n",
    "        de_married_list = []\n",
    "        de_unmarried_list = []\n",
    "        for i in range(len(agg_opt_res)):\n",
    "            dimensions_name = agg_opt_res[i][0].columns[0]\n",
    "            married_combined_df = agg_opt_res[i][0]\n",
    "            unmarried_combined_df = agg_opt_res[i][1]\n",
    "            decomposeAggTable(dimensions_name, married_combined_df, de_married_list)\n",
    "            decomposeAggTable(dimensions_name, unmarried_combined_df, de_unmarried_list)\n",
    "\n",
    "        upper_bound_map = dict()\n",
    "        lower_bound_map = dict()\n",
    "    \n",
    "        # Normalize aggregate data and compute utility score.\n",
    "        for (attribute, measurement, function), df_married, df_unmarried in zip(candidate_views, de_married_list, de_unmarried_list):            \n",
    "            vid = (attribute, measurement, function)\n",
    "        \n",
    "            # Collects unique values of this attribute from both married and unmarried dataframes.\n",
    "            all_attribute_values = set(df_married[attribute].unique()).union(set(df_unmarried[attribute].unique()))\n",
    "\n",
    "            # print(vid, df_married, df_unmarried)\n",
    "            \n",
    "            # Organize data and ensure all possible attribute values are included using reindex function.\n",
    "            grouped_married = df_married.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "            grouped_unmarried = df_unmarried.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "            \n",
    "            # Compute sum for normalization.\n",
    "            total_married = grouped_married[f'{function}_{measurement}'].sum()\n",
    "            total_unmarried = grouped_unmarried[f'{function}_{measurement}'].sum()\n",
    "    \n",
    "            # Normalization\n",
    "            p = (grouped_married / total_married).fillna(0).values.flatten()\n",
    "            q = (grouped_unmarried / total_unmarried).fillna(0).values.flatten()\n",
    "    \n",
    "            # Compute the KL score for current partition.\n",
    "            curr_score = kl_divergence(p, q)\n",
    "            view_scores[vid] = view_scores[vid] + [curr_score]\n",
    "\n",
    "            # Computing epsilon_m and estimated mean.\n",
    "            N = num_phases\n",
    "            m = len(view_scores[vid])\n",
    "            estimated_mean = np.mean(view_scores[vid])\n",
    "            error_range = epsilon_m(m, N)\n",
    "\n",
    "            # Save upper bound and lower bound into map for future references.\n",
    "            upper_bound_map[vid] = estimated_mean + error_range\n",
    "            lower_bound_map[vid] = estimated_mean - error_range\n",
    "\n",
    "        # Get the k-th highest lower bound.\n",
    "        kth_lower_bound = list(sorted(lower_bound_map.items(), key=lambda x: x[1], reverse=True))[k]\n",
    "        \n",
    "        # Prune views that has an upper bound lower than the k-th highest lower bound.\n",
    "        for vid, upper_bound in upper_bound_map.items():\n",
    "            if upper_bound < kth_lower_bound[1]:\n",
    "                candidate_views.remove(vid)  # Remove this view from candidate views.\n",
    "                del view_scores[vid]  # Remove view scores history as well.\n",
    "\n",
    "        # This is a debug log. Uncomment it only when needed.\n",
    "        # print('End of phase', phase, 'having candidate views', len(candidate_views))\n",
    "\n",
    "    final_view_scores = dict()\n",
    "    for vid in candidate_views:\n",
    "        final_view_scores[vid] = np.mean(view_scores[vid])\n",
    "\n",
    "    # Print top-k results.\n",
    "    top_scores = sorted(final_view_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    for key, score in top_scores:\n",
    "        print(f\"View: {key}, Score: {score}\")\n",
    "\n",
    "    return top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Measure execution time of baseline.\n",
    "start_time = time.time()\n",
    "result_one = baseline()\n",
    "end_time = time.time()\n",
    "duration_one = end_time - start_time\n",
    "print(f\"Baseline: {duration_one} seconds\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Measure execution time of sharing-based optimizations.\n",
    "start_time = time.time()\n",
    "result_one = sharing_based_optimizations()\n",
    "end_time = time.time()\n",
    "duration_one = end_time - start_time\n",
    "print(f\"Sharing-based: {duration_one} seconds\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Measure execution time of pruning-based optimizations.\n",
    "start_time = time.time()\n",
    "result_two = pruning_based_optimizations()\n",
    "end_time = time.time()\n",
    "duration_two = end_time - start_time\n",
    "print(f\"Pruning-based: {duration_two} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
