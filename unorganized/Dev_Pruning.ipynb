{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5523b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535ca410",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:1234@localhost:5432/census_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68dff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/_cc1rhzx0m75dzxj31n76d2m0000gn/T/ipykernel_47948/84426944.py:2: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  result = engine.execute(\"SELECT 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = engine.execute(\"SELECT 1\")\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0aa2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    epsilon = 1e-10  # 避免数学错误的小常数\n",
    "    p = np.where(p == 0, epsilon, p)\n",
    "    q = np.where(q == 0, epsilon, q)\n",
    "    return np.sum(p * np.log(p / q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a3916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharing opt\n",
    "# The following functions can generated the combined queries\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "function_lst = ['SUM', 'COUNT', 'AVG', 'MAX', 'MIN']\n",
    "\n",
    "select_pattern = r\"SELECT\\s+(.*?)\\s+FROM\"\n",
    "from_pattern = r\"FROM\\s+(.*?)(?:\\s+WHERE|\\s+GROUP\\s+BY|$)\"\n",
    "where_pattern = r\"WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|$)\"\n",
    "group_by_pattern = r\"GROUP\\s+BY\\s+(.*)\"\n",
    "\n",
    "\n",
    "def parseQuery(query):\n",
    "    select_match = re.search(select_pattern, query, re.IGNORECASE)\n",
    "    from_match = re.search(from_pattern, query, re.IGNORECASE)\n",
    "    where_match = re.search(where_pattern, query, re.IGNORECASE)\n",
    "    group_by_match = re.search(group_by_pattern, query, re.IGNORECASE)\n",
    "\n",
    "    select_items = select_match.group(1) if select_match else \"\"\n",
    "    from_items = from_match.group(1) if from_match else \"\"\n",
    "    where_items = where_match.group(1) if where_match else \"\"\n",
    "    group_by_items = group_by_match.group(1) if group_by_match else \"\"\n",
    "\n",
    "    return select_items, from_items, where_items, group_by_items\n",
    "\n",
    "\n",
    "def parseSelectItems(select_items):\n",
    "    components = select_items.split(',')\n",
    "    attribute = None\n",
    "    measures, functions = [], []\n",
    "\n",
    "    for component in components:\n",
    "        component = component.strip()\n",
    "        component_upper = component.upper()\n",
    "        for func in function_lst:\n",
    "            if func.upper() in component_upper:\n",
    "                measure_match = re.search(rf\"{func}\\((.*?)\\)\", component, re.IGNORECASE)\n",
    "                if measure_match:\n",
    "                    measure = measure_match.group(1)\n",
    "                    measures.append(measure)\n",
    "                    functions.append(func)\n",
    "                break\n",
    "        else:\n",
    "            attribute = component\n",
    "\n",
    "    return attribute, measures, functions\n",
    "\n",
    "def combineAggregates(queries, partition=None):\n",
    "    # Group queries by FROM and WHERE clauses for potential combination\n",
    "    query_groups = defaultdict(list)\n",
    "    for query in queries:\n",
    "        select_items, from_clause, where_clause, group_by_items = parseQuery(query.rstrip(';'))\n",
    "        attribute, measures, functions = parseSelectItems(select_items)\n",
    "        key = (from_clause, where_clause)\n",
    "        query_groups[key].append((group_by_items, measures, functions, attribute))\n",
    "\n",
    "    combined_queries = []\n",
    "    for (from_clause, where_clause), group in query_groups.items():\n",
    "        combined_by_group = defaultdict(list)\n",
    "        for group_by_items, measures, functions, attribute in group:\n",
    "            combined_by_group[group_by_items].append((measures, functions, attribute))\n",
    "\n",
    "        for group_by_items, details in combined_by_group.items():\n",
    "            select_parts = []\n",
    "            for measures, functions, attribute in details:\n",
    "                for measure, function in zip(measures, functions):\n",
    "                    select_parts.append(f\"{function}({measure}) AS {function}_{measure}\")\n",
    "            select_clause = ', '.join(select_parts)\n",
    "            if partition is not None:\n",
    "                partition_clause = f\"partition_id = {partition}\"\n",
    "                where_clause = f\"{where_clause} AND {partition_clause}\" if where_clause else partition_clause\n",
    "            where_clause = f\"WHERE {where_clause}\" if where_clause else \"\"\n",
    "            group_by_clause = f\"GROUP BY {group_by_items}\" if group_by_items else \"\"\n",
    "            query = f\"SELECT {group_by_items}, {select_clause} FROM {from_clause} {where_clause} {group_by_clause};\"\n",
    "            combined_queries.append(query)\n",
    "\n",
    "    return combined_queries\n",
    "\n",
    "\n",
    "def decomposeAggTable(dimensions_name, combined_dataframe, individual_list):\n",
    "    measurements = set(col.split('_')[1] for col in combined_dataframe.columns if '_' in col)\n",
    "\n",
    "    # Create a dictionary to store each separate DataFrame\n",
    "    separated_tables = {}\n",
    "\n",
    "    for column in combined_dataframe.columns:\n",
    "        if column != dimensions_name:\n",
    "            # Properly split the column name\n",
    "            parts = column.split('_')\n",
    "            func = parts[0]  # The function is always the first part\n",
    "            measure = '_'.join(parts[1:])  # The rest is the measurement name\n",
    "\n",
    "            table_name = f\"{func}_{measure}\"  # Create a unique table name\n",
    "\n",
    "            # Create a new DataFrame for this specific measurement and function\n",
    "            separated_tables[table_name] = combined_dataframe[[dimensions_name, column]].copy()\n",
    "            separated_tables[table_name].rename(columns={column: table_name}, inplace=True)\n",
    "\n",
    "    # Printing or exporting the separated tables\n",
    "    for table_name, df in separated_tables.items():\n",
    "        individual_list.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5f9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义维度、度量和聚合函数\n",
    "import itertools\n",
    "\n",
    "dimensions = ['workclass', 'education', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
    "measurements = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "aggregate_functions = ['count', 'max', 'min', 'sum', 'avg']\n",
    "\n",
    "# 初始化视图\n",
    "views = {}\n",
    "\n",
    "for dimension, measurement, function in itertools.product(dimensions, measurements, aggregate_functions):\n",
    "    vid = f\"{dimension}_{measurement}_{function}\"\n",
    "    views[vid] = (dimension, measurement, function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76664e80",
   "metadata": {},
   "source": [
    "## Generate the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbbbc80-59dd-4cf4-a172-7c269e8fdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(candidate_views):\n",
    "    \"\"\"\n",
    "    This function is used to generate queries based off the current candidate views.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_views\n",
    "        The modified dictionary of views after pruning views in the end of every phase.\n",
    "    \"\"\"\n",
    "    married_query_list = []\n",
    "    unmarried_query_list = []\n",
    "    for key, (attribute, measurement, function) in candidate_views.items():\n",
    "        married_query_list.append(f\"SELECT {attribute}, {function}({measurement}) AS {function}_{measurement} FROM married_data WHERE {attribute} IS NOT NULL GROUP BY {attribute};\")\n",
    "        unmarried_query_list.append(f\"SELECT {attribute}, {function}({measurement}) AS {function}_{measurement} FROM unmarried_data WHERE {attribute} IS NOT NULL GROUP BY {attribute};\")\n",
    "    return married_query_list, unmarried_query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d8b2b",
   "metadata": {},
   "source": [
    "## Generate and Decompose the aggregation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60579c55-34d4-4ca4-96d8-d85296694757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "delta = 0.0001\n",
    "\n",
    "\n",
    "def epsilon_m(m: int, N: int):\n",
    "    \"\"\"\n",
    "    Calculate the epsilon value for the confidence interval pruning method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        The number of items in the sample.\n",
    "    N : int\n",
    "        The number of items in the population.\n",
    "    \"\"\"\n",
    "    upper_left = 1 - (m - 1) / N\n",
    "    # Using log log m is problematic when m=1. So we change it to log m for now.\n",
    "    upper_right = 2 * np.log(m) + np.log((np.pi ** 2) / (3 * delta))\n",
    "    bottom = 2 * m\n",
    "    return np.sqrt(upper_left * upper_right / bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_phases = 5\n",
    "k = 5\n",
    "\n",
    "\n",
    "def pruning_based_optimization():\n",
    "    # Make a copy of the views dictionary so that it won't change the original data.\n",
    "    candidate_views = copy.deepcopy(views)\n",
    "\n",
    "    # Storing scores across phases, where key = a_m_f and value = KL.\n",
    "    view_scores = defaultdict(list)\n",
    "    \n",
    "    for phase in range(num_phases):\n",
    "        # Get raw queries (separate statements).\n",
    "        raw_queries_married, raw_queries_unmarried = generate_queries(candidate_views)\n",
    "\n",
    "        # Combine statements together (Section 4.1).\n",
    "        combined_queries_married = combineAggregates(raw_queries_married, partition=phase)\n",
    "        combined_queries_unmarried = combineAggregates(raw_queries_unmarried, partition=phase)\n",
    "\n",
    "        # Run combined queries for both married and unmarried data.\n",
    "        agg_opt_res = []\n",
    "        for i in range(len(combined_queries_married)):\n",
    "            df_married_agg_opt = pd.read_sql_query(combined_queries_married[i], engine)\n",
    "            df_unmarried_agg_opt = pd.read_sql_query(combined_queries_unmarried[i], engine)\n",
    "            agg_opt_res.append((df_married_agg_opt, df_unmarried_agg_opt))\n",
    "\n",
    "        # Decompose combined queries back into original views.\n",
    "        de_married_list = []\n",
    "        de_unmarried_list = []\n",
    "        for i in range(len(agg_opt_res)):\n",
    "            dimensions_name = agg_opt_res[i][0].columns[0]\n",
    "            married_combined_df = agg_opt_res[i][0]\n",
    "            unmarried_combined_df = agg_opt_res[i][1]\n",
    "            decomposeAggTable(dimensions_name, married_combined_df, de_married_list)\n",
    "            decomposeAggTable(dimensions_name, unmarried_combined_df, de_unmarried_list)\n",
    "\n",
    "        print('len:', len(de_married_list), len(candidate_views))\n",
    "\n",
    "        upper_bound_map = dict()\n",
    "        lower_bound_map = dict()\n",
    "    \n",
    "        # Normalize aggregate data and compute utility score.\n",
    "        for query_idx, (attribute, measurement, function) in enumerate(candidate_views.values()):\n",
    "            vid = f\"{attribute}_{measurement}_{function}\"\n",
    "            \n",
    "            df_married = de_married_list[query_idx]\n",
    "            df_unmarried = de_unmarried_list[query_idx]\n",
    "        \n",
    "            all_attribute_values = set(df_married[attribute].unique()).union(set(df_unmarried[attribute].unique()))\n",
    "    \n",
    "            grouped_married = df_married.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "            grouped_unmarried = df_unmarried.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "    \n",
    "            total_married = grouped_married[f'{function}_{measurement}'].sum()\n",
    "            total_unmarried = grouped_unmarried[f'{function}_{measurement}'].sum()\n",
    "    \n",
    "            # Normalization\n",
    "            p = (grouped_married / total_married).fillna(0).values.flatten()\n",
    "            q = (grouped_unmarried / total_unmarried).fillna(0).values.flatten()\n",
    "    \n",
    "            # Compute the KL score for current partition.\n",
    "            curr_score = kl_divergence(p, q)\n",
    "            view_scores[vid] = view_scores[vid] + [curr_score]\n",
    "\n",
    "            # Computing epsilon_m.\n",
    "            N = num_phases\n",
    "            m = len(view_scores[vid])\n",
    "            estimated_mean = np.mean(view_scores[vid])\n",
    "            error_range = epsilon_m(m, N)\n",
    "\n",
    "            upper_bound_map[vid] = estimated_mean + error_range\n",
    "            lower_bound_map[vid] = estimated_mean - error_range\n",
    "\n",
    "        kth_lower_bound = list(sorted(lower_bound_map.items(), key=lambda x: x[1], reverse=True))[k]\n",
    "        # Prune views that has an upper bound lower than the k-th highest lower bound.\n",
    "        for vid, upper_bound in upper_bound_map.items():\n",
    "            if upper_bound < kth_lower_bound[1]:\n",
    "                del candidate_views[vid]\n",
    "                del view_scores[vid]\n",
    "                # print('Dropped:', vid)\n",
    "\n",
    "        print('End of phase', phase, 'having candidate views', len(candidate_views))\n",
    "\n",
    "    final_view_scores = dict()\n",
    "    for vid in candidate_views.keys():\n",
    "        final_view_scores[vid] = np.mean(view_scores[vid])\n",
    "\n",
    "    print(view_scores['relationship_age_sum'])\n",
    "    top_scores = sorted(final_view_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for key, score in top_scores:\n",
    "        print(f\"View: {key}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f811218",
   "metadata": {},
   "source": [
    "# Baseline non-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032b0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline():\n",
    "    # 这一部分可能结果有问题，逻辑没问题(就是把结果里面每一个view查到的结果，求归一化后求K-L偏差，最后排序找到前5个，输出分数)\n",
    "    results = []\n",
    "    raw_queries_married, raw_queries_unmarried = generate_queries(views)\n",
    "    for married_query, unmarried_query in zip(raw_queries_married, raw_queries_unmarried):\n",
    "        # 执行已婚数据查询并直接读取到 DataFrame\n",
    "        df_married = pd.read_sql_query(married_query, engine)\n",
    "\n",
    "        # 执行未婚数据查询并直接读取到 DataFrame\n",
    "        df_unmarried = pd.read_sql_query(unmarried_query, engine)\n",
    "\n",
    "        # 将结果存储为元组\n",
    "        results.append((df_married, df_unmarried))\n",
    "\n",
    "\n",
    "    view_scores = {}  # 初始化空字典，将用来存储三元组和对应的K-L散度\n",
    "\n",
    "    # 修正并重新计算分数\n",
    "    for query_idx, (attribute, measurement, function) in enumerate(views.values()):\n",
    "        df_married = results[query_idx][0]\n",
    "        df_unmarried = results[query_idx][1]\n",
    "\n",
    "        all_attribute_values = set(df_married[attribute].unique()).union(set(df_unmarried[attribute].unique()))\n",
    "\n",
    "        grouped_married = df_married.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "        grouped_unmarried = df_unmarried.groupby(attribute).agg({f'{function}_{measurement}': 'sum'}).reindex(all_attribute_values, fill_value=0)\n",
    "\n",
    "        total_married = grouped_married[f'{function}_{measurement}'].sum()\n",
    "        total_unmarried = grouped_unmarried[f'{function}_{measurement}'].sum()\n",
    "\n",
    "        # 归一化以获得概率分布\n",
    "        p = (grouped_married / total_married).fillna(0).values.flatten()\n",
    "        q = (grouped_unmarried / total_unmarried).fillna(0).values.flatten()\n",
    "\n",
    "        # 计算 K-L 散度并保存\n",
    "        score = kl_divergence(p, q)\n",
    "        view_scores[(attribute, measurement, function)] = score\n",
    "\n",
    "    print(view_scores['relationship', 'age', 'sum'])\n",
    "    # 排序并打印top 5\n",
    "    top_scores = sorted(view_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for key, score in top_scores:\n",
    "        print(f\"View: {key}, Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd18a5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 240 240\n",
      "End of phase 0 having candidate views 17\n",
      "len: 17 17\n",
      "End of phase 1 having candidate views 13\n",
      "len: 13 13\n",
      "End of phase 2 having candidate views 13\n",
      "len: 13 13\n",
      "End of phase 3 having candidate views 13\n",
      "len: 13 13\n",
      "End of phase 4 having candidate views 13\n",
      "[18.62180469148511, 18.56626478937287, 18.522641288417436, 18.47121699308135, 18.89536108456243]\n",
      "View: relationship_capital_gain_sum, Score: 21.23768404937885\n",
      "View: relationship_capital_loss_sum, Score: 20.439943294515913\n",
      "View: relationship_hours_per_week_sum, Score: 19.312089411774245\n",
      "View: relationship_education_num_sum, Score: 19.163364340487533\n",
      "View: relationship_age_count, Score: 18.81313103895464\n",
      "Approach Two Execution Time: 5.547677755355835 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Measure execution time of approach_one\n",
    "# start_time = time.time()\n",
    "# result_one = baseline()\n",
    "# end_time = time.time()\n",
    "# duration_one = end_time - start_time\n",
    "# print(f\"Baseline Time: {duration_one} seconds\")\n",
    "\n",
    "# Measure execution time of approach_two\n",
    "start_time = time.time()\n",
    "result_two = pruning_based_optimization()\n",
    "end_time = time.time()\n",
    "duration_two = end_time - start_time\n",
    "print(f\"Approach Two Execution Time: {duration_two} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d1a70-fc1d-4180-8cb2-1ef841b2f138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
